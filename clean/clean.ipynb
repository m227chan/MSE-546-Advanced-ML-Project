{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo Bank Data Preparation Notebook\n",
    "\n",
    "The purpose of this notebook is to clean the dataset and create the features used in the train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load historical (2008-2023) dataset used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the combined DataFrame: 3926058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for year in range(2008, 2024):\n",
    "    file_path = f\"../data/train_{year}.parquet\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read file {file_path}: {e}\")\n",
    "if dataframes:\n",
    "    combined_train_raw_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"Total rows in the combined DataFrame: {len(combined_train_raw_df)}\")\n",
    "else:\n",
    "    print(\"No files were successfully read.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360472\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"../data/test.parquet\"\n",
    "test_raw_df = pd.read_parquet(file_path)\n",
    "print(len(test_raw_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unrealistic records of atm transfer out\n",
    "\n",
    "# # current justification: it's not realisitc that someone can transfer money out of an ATM thousands of times in single day also it accounts for tiny \n",
    "# # TODO: find a source to backup the reasoning for removing this data\n",
    "\n",
    "# initial_count = len(combined_train_raw_df)\n",
    "\n",
    "# # drop records with more than 10 atm transfer outs\n",
    "# filtered_df = combined_train_raw_df.drop(combined_train_raw_df[combined_train_raw_df['atm_transfer_out'] > 20].index)\n",
    "\n",
    "# # Calculate dropped records\n",
    "# dropped_count = initial_count - len(filtered_df)\n",
    "# percentage_dropped = (dropped_count / initial_count) * 100\n",
    "\n",
    "# print(f\"Records dropped: {dropped_count}\")\n",
    "# print(f\"Percentage of dataset removed: {percentage_dropped:.2f}%\")\n",
    "\n",
    "# combined_train_raw_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unrealistic records of atm transfer out\n",
    "\n",
    "# # current justification: it's not realisitc that someone can transfer money out of an ATM thousands of times in single day also it accounts for tiny \n",
    "# # TODO: find a source to backup the reasoning for removing this data\n",
    "\n",
    "# initial_count = len(test_raw_df)\n",
    "\n",
    "# # drop records with more than 10 atm transfer outs\n",
    "# filtered_df = test_raw_df.drop(test_raw_df[test_raw_df['atm_transfer_out'] > 20].index)\n",
    "\n",
    "# # Calculate dropped records\n",
    "# dropped_count = initial_count - len(filtered_df)\n",
    "# percentage_dropped = (dropped_count / initial_count) * 100\n",
    "\n",
    "# print(f\"Records dropped: {dropped_count}\")\n",
    "# print(f\"Percentage of dataset removed: {percentage_dropped:.2f}%\")\n",
    "\n",
    "# test_raw_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unrealistic records of atm transfer in\n",
    "\n",
    "# # current justification: it's not realisitc that someone can transfer money out of an ATM thousands of times in single day also it accounts for tiny \n",
    "# # TODO: find a source to backup the reasoning for removing this data\n",
    "\n",
    "# # drop records with more than 10 atm transfer ins\n",
    "# filtered_df = combined_train_raw_df.drop(combined_train_raw_df[combined_train_raw_df['atm_transfer_in'] > 20].index)\n",
    "\n",
    "# # Calculate dropped records\n",
    "# dropped_count = initial_count - len(filtered_df)\n",
    "# percentage_dropped = (dropped_count / initial_count) * 100\n",
    "\n",
    "# print(f\"Records dropped: {dropped_count}\")\n",
    "# print(f\"Percentage of dataset removed: {percentage_dropped:.2f}%\")\n",
    "\n",
    "# combined_train_raw_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unrealistic records of atm transfer in\n",
    "\n",
    "# # current justification: it's not realisitc that someone can transfer money out of an ATM thousands of times in single day also it accounts for tiny \n",
    "# # TODO: find a source to backup the reasoning for removing this data\n",
    "\n",
    "# # drop records with more than 10 atm transfer ins\n",
    "# filtered_df = test_raw_df.drop(test_raw_df[test_raw_df['atm_transfer_in'] > 20].index)\n",
    "\n",
    "# # Calculate dropped records\n",
    "# dropped_count = initial_count - len(filtered_df)\n",
    "# percentage_dropped = (dropped_count / initial_count) * 100\n",
    "\n",
    "# print(f\"Records dropped: {dropped_count}\")\n",
    "# print(f\"Percentage of dataset removed: {percentage_dropped:.2f}%\")\n",
    "\n",
    "# test_raw_df = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training dataset (customer_id level data)\n",
    "\n",
    "Churn definition:\n",
    "- No interactions with the bank for at 18 or more months since the max date in the dataset (2023-12-31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "combined_train_raw_df['date'] = pd.to_datetime(combined_train_raw_df['date'])\n",
    "\n",
    "max_date_train = max(combined_train_raw_df['date'])\n",
    "print(max_date_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "test_raw_df['date'] = pd.to_datetime(test_raw_df['date'])\n",
    "\n",
    "max_date_test = max(test_raw_df['date'])\n",
    "print(max_date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.94% of distinct customers since 2008-2023 have churned under our definition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>churn_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  churn_status\n",
       "0            1         False\n",
       "1            2          True\n",
       "2            3          True\n",
       "3            4          True\n",
       "4            5         False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last interaction date for each customer\n",
    "last_interaction = combined_train_raw_df.groupby('customer_id')['date'].max()\n",
    "\n",
    "# Define the churn threshold\n",
    "churn_date = pd.Timestamp(max_date_train) \n",
    "cutoff_date = churn_date - pd.DateOffset(months=18)\n",
    "\n",
    "churn_status = last_interaction < cutoff_date\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = churn_status.reset_index(name='churn_status')\n",
    "\n",
    "churn_percentage = (df_train['churn_status'].sum() / len(df_train)) * 100\n",
    "print(f\"{churn_percentage:.2f}% of distinct customers since 2008-2023 have churned under our definition\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.96% of distinct customers in 2024 have churned under our definition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>churn_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  churn_status\n",
       "0            1         False\n",
       "1            2         False\n",
       "2            3          True\n",
       "3            4          True\n",
       "4            5          True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last interaction date for each customer\n",
    "last_interaction = test_raw_df.groupby('customer_id')['date'].max()\n",
    "\n",
    "# Define the churn threshold\n",
    "churn_date = pd.Timestamp(max_date_test) # Training the model up to the \n",
    "cutoff_date = churn_date - pd.DateOffset(months=18)\n",
    "\n",
    "# Determine churn status (True if last interaction was before cutoff_date)\n",
    "churn_status = last_interaction < cutoff_date\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_test = churn_status.reset_index(name='churn_status')\n",
    "\n",
    "churn_percentage = (df_test['churn_status'].sum() / len(df_test)) * 100\n",
    "print(f\"{churn_percentage:.2f}% of distinct customers in 2024 have churned under our definition\")\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Features:\n",
    "- customer age\n",
    "- country - useless\n",
    "- account age\n",
    "- customer job category\n",
    "- from competitor - useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "Both False                 2655657\n",
      "Fraud False, Churn True    1270374\n",
      "Both True                       23\n",
      "Fraud True, Churn False          4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# This code compares our definition of churn to the instances of churn due to fraud (since we can assume that these are ACTUAL cases where a customer churned)\n",
    "\n",
    "# Merge dataframes on customer_id\n",
    "merged_df = combined_train_raw_df.merge(df_train, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Categorizing based on churn_due_to_fraud and churn_status\n",
    "conditions = [\n",
    "    (merged_df[\"churn_due_to_fraud\"] == True) & (merged_df[\"churn_status\"] == True),\n",
    "    (merged_df[\"churn_due_to_fraud\"] == False) & (merged_df[\"churn_status\"] == False),\n",
    "    (merged_df[\"churn_due_to_fraud\"] == True) & (merged_df[\"churn_status\"] == False),\n",
    "    (merged_df[\"churn_due_to_fraud\"] == False) & (merged_df[\"churn_status\"] == True)\n",
    "]\n",
    "\n",
    "categories = [\"Both True\", \"Both False\", \"Fraud True, Churn False\", \"Fraud False, Churn True\"]\n",
    "\n",
    "merged_df[\"Category\"] = np.select(conditions, categories, default=\"Unknown\")\n",
    "category_counts = merged_df[\"Category\"].value_counts()\n",
    "\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "Both False                 1284242\n",
      "Fraud False, Churn True      76175\n",
      "Fraud True, Churn False         48\n",
      "Both True                        7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# This code compares our definition of churn to the instances of churn due to fraud (since we can assume that these are ACTUAL cases where a customer churned)\n",
    "\n",
    "# Merge dataframes on customer_id\n",
    "merged_df = test_raw_df.merge(df_test, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Categorizing based on churn_due_to_fraud and churn_status\n",
    "conditions = [\n",
    "    (merged_df[\"churn_due_to_fraud\"] == True) & (merged_df[\"churn_status\"] == True),\n",
    "    (merged_df[\"churn_due_to_fraud\"] == False) & (merged_df[\"churn_status\"] == False),\n",
    "    (merged_df[\"churn_due_to_fraud\"] == True) & (merged_df[\"churn_status\"] == False),\n",
    "    (merged_df[\"churn_due_to_fraud\"] == False) & (merged_df[\"churn_status\"] == True)\n",
    "]\n",
    "\n",
    "categories = [\"Both True\", \"Both False\", \"Fraud True, Churn False\", \"Fraud False, Churn True\"]\n",
    "\n",
    "merged_df[\"Category\"] = np.select(conditions, categories, default=\"Unknown\")\n",
    "category_counts = merged_df[\"Category\"].value_counts()\n",
    "\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute age as of 2023-12-31\n",
    "# combined_train_raw_df['date_of_birth'] = pd.to_datetime(combined_train_raw_df['date_of_birth'])\n",
    "# age = (pd.Timestamp('2023-12-31') - combined_train_raw_df.groupby('customer_id')['date_of_birth'].min()).dt.days // 365\n",
    "\n",
    "# # Convert to DataFrame and merge with df_train\n",
    "# age_df = age.reset_index(name='age')\n",
    "# df_train = df_train.merge(age_df, on='customer_id')\n",
    "# len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute age as of 2023-12-31\n",
    "# test_raw_df['date_of_birth'] = pd.to_datetime(test_raw_df['date_of_birth'])\n",
    "# age = (pd.Timestamp('2023-12-31') - test_raw_df.groupby('customer_id')['date_of_birth'].min()).dt.days // 365\n",
    "\n",
    "# # Convert to DataFrame and merge with df_test\n",
    "# age_df = age.reset_index(name='age')\n",
    "# df_test = df_test.merge(age_df, on='customer_id')\n",
    "# len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encode the 'country' column\n",
    "# country_one_hot = pd.get_dummies(combined_train_raw_df['country'], prefix='country')\n",
    "\n",
    "# # Merge one-hot encoded 'country' columns into df_train\n",
    "# df_train = pd.merge(df_train, combined_train_raw_df[['customer_id']].drop_duplicates().merge(country_one_hot, left_index=True, right_index=True), on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encode the 'country' column\n",
    "# country_one_hot = pd.get_dummies(test_raw_df['country'], prefix='country')\n",
    "\n",
    "# # Merge one-hot encoded 'country' columns into df_test\n",
    "# df_test = pd.merge(df_test, test_raw_df[['customer_id']].drop_duplicates().merge(country_one_hot, left_index=True, right_index=True), on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# TRANSACTION FEATURES\n",
    "# ==================================================================\n",
    "# 1. 30d Crypto Volume\n",
    "combined_train_raw_df['daily_crypto_volume'] = combined_train_raw_df['crypto_in_volume'] + combined_train_raw_df['crypto_out_volume']\n",
    "\n",
    "crypto_30d = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30D', on='date')\n",
    "    ['daily_crypto_volume']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='30d_crypto_volume')\n",
    ")\n",
    "\n",
    "# 2. 90d ATM Withdrawal Trend (Slope)\n",
    "def calculate_trend(series):\n",
    "    if len(series) < 2: return np.nan\n",
    "    x = np.arange(len(series))\n",
    "    return np.polyfit(x, series, 1)[0]\n",
    "\n",
    "atm_trend_90d = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('90D', on='date')\n",
    "    ['atm_transfer_out']\n",
    "    .apply(calculate_trend, raw=True)\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='90d_atm_trend')\n",
    ")\n",
    "\n",
    "# 3. Days Since Last Bank Transfer\n",
    "last_transfer = (\n",
    "    combined_train_raw_df[\n",
    "        (combined_train_raw_df['bank_transfer_in'] > 0) |\n",
    "        (combined_train_raw_df['bank_transfer_out'] > 0)\n",
    "    ]\n",
    "    .groupby('customer_id')['date'].max()\n",
    "    .reset_index(name='last_transfer_date')\n",
    ")\n",
    "last_transfer['days_since_transfer'] = (\n",
    "    pd.to_datetime(cutoff_date) - last_transfer['last_transfer_date']\n",
    ").dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# TRANSACTION FEATURES\n",
    "# ==================================================================\n",
    "# 1. 30d Crypto Volume\n",
    "test_raw_df['daily_crypto_volume'] = test_raw_df['crypto_in_volume'] + test_raw_df['crypto_out_volume']\n",
    "\n",
    "crypto_30d = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30D', on='date')\n",
    "    ['daily_crypto_volume']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='30d_crypto_volume')\n",
    ")\n",
    "\n",
    "# 2. 90d ATM Withdrawal Trend (Slope)\n",
    "def calculate_trend(series):\n",
    "    if len(series) < 2: return np.nan\n",
    "    x = np.arange(len(series))\n",
    "    return np.polyfit(x, series, 1)[0]\n",
    "\n",
    "atm_trend_90d = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('90D', on='date')\n",
    "    ['atm_transfer_out']\n",
    "    .apply(calculate_trend, raw=True)\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='90d_atm_trend')\n",
    ")\n",
    "\n",
    "# 3. Days Since Last Bank Transfer\n",
    "last_transfer = (\n",
    "    test_raw_df[\n",
    "        (test_raw_df['bank_transfer_in'] > 0) |\n",
    "        (test_raw_df['bank_transfer_out'] > 0)\n",
    "    ]\n",
    "    .groupby('customer_id')['date'].max()\n",
    "    .reset_index(name='last_transfer_date')\n",
    ")\n",
    "last_transfer['days_since_transfer'] = (\n",
    "    pd.to_datetime(cutoff_date) - last_transfer['last_transfer_date']\n",
    ").dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email CSAT (60d):\n",
      "    customer_id  avg_email_csat_60d\n",
      "0            1                 7.0\n",
      "1            2                 7.0\n",
      "2            3                 7.0\n",
      "3            4                 7.0\n",
      "4            5                 7.0\n",
      "\n",
      "Complaints (90d):\n",
      "    customer_id  total_complaints_90d\n",
      "0            1                   0.0\n",
      "1            2                   0.0\n",
      "2            3                   0.0\n",
      "3            4                   0.0\n",
      "4            5                   0.0\n",
      "\n",
      "Phone Touchpoints (30d):\n",
      "    customer_id  phone_touchpoints_30d\n",
      "0            1                    0.0\n",
      "1            2                    0.0\n",
      "2            3                    0.0\n",
      "3            4                    0.0\n",
      "4            5                    0.0\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# SUPPORT INTERACTION FEATURES\n",
    "# ==================================================================\n",
    "# 1. Extract Email CSAT Scores\n",
    "combined_train_raw_df['email_csat'] = combined_train_raw_df['csat_scores'].apply(\n",
    "    lambda x: x.get('email', np.nan) if isinstance(x, dict) else np.nan\n",
    ")\n",
    "\n",
    "# 2. Sort data chronologically\n",
    "combined_train_raw_df = combined_train_raw_df.sort_values(['customer_id', 'date'])\n",
    "\n",
    "# 3. Avg Email CSAT (60d)\n",
    "email_csat_60d = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('60D', on='date')['email_csat']\n",
    "    .mean()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='avg_email_csat_60d')\n",
    ")\n",
    "\n",
    "# 4. Total Complaints (90d)\n",
    "complaints_90d = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('90D', on='date')['complaints']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='total_complaints_90d')\n",
    ")\n",
    "\n",
    "# 5. Phone Touchpoints (30d)\n",
    "combined_train_raw_df['phone_touchpoints'] = combined_train_raw_df['touchpoints'].apply(\n",
    "    lambda x: x.count('phone') if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "phone_touch_30d = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30D', on='date')['phone_touchpoints']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='phone_touchpoints_30d')\n",
    ")\n",
    "\n",
    "# Fill NA values for customers with no support interactions\n",
    "email_csat_60d['avg_email_csat_60d'] = email_csat_60d['avg_email_csat_60d'].fillna(\n",
    "    combined_train_raw_df['email_csat'].median()\n",
    ")\n",
    "complaints_90d['total_complaints_90d'] = complaints_90d['total_complaints_90d'].fillna(0)\n",
    "phone_touch_30d['phone_touchpoints_30d'] = phone_touch_30d['phone_touchpoints_30d'].fillna(0)\n",
    "\n",
    "print(\"Email CSAT (60d):\\n\", email_csat_60d.head())\n",
    "print(\"\\nComplaints (90d):\\n\", complaints_90d.head())\n",
    "print(\"\\nPhone Touchpoints (30d):\\n\", phone_touch_30d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email CSAT (60d):\n",
      "    customer_id  avg_email_csat_60d\n",
      "0            1                 6.0\n",
      "1            2                 6.0\n",
      "2            3                 6.0\n",
      "3            4                 6.0\n",
      "4            5                 6.0\n",
      "\n",
      "Complaints (90d):\n",
      "    customer_id  total_complaints_90d\n",
      "0            1                   0.0\n",
      "1            2                   0.0\n",
      "2            3                   0.0\n",
      "3            4                   0.0\n",
      "4            5                   0.0\n",
      "\n",
      "Phone Touchpoints (30d):\n",
      "    customer_id  phone_touchpoints_30d\n",
      "0            1                    0.0\n",
      "1            2                    0.0\n",
      "2            3                    0.0\n",
      "3            4                    0.0\n",
      "4            5                    0.0\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# SUPPORT INTERACTION FEATURES\n",
    "# ==================================================================\n",
    "# 1. Extract Email CSAT Scores\n",
    "test_raw_df['email_csat'] = test_raw_df['csat_scores'].apply(\n",
    "    lambda x: x.get('email', np.nan) if isinstance(x, dict) else np.nan\n",
    ")\n",
    "\n",
    "# 2. Sort data chronologically\n",
    "test_raw_df = test_raw_df.sort_values(['customer_id', 'date'])\n",
    "\n",
    "# 3. Avg Email CSAT (60d)\n",
    "email_csat_60d = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('60D', on='date')['email_csat']\n",
    "    .mean()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='avg_email_csat_60d')\n",
    ")\n",
    "\n",
    "# 4. Total Complaints (90d)\n",
    "complaints_90d = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('90D', on='date')['complaints']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='total_complaints_90d')\n",
    ")\n",
    "\n",
    "# 5. Phone Touchpoints (30d)\n",
    "test_raw_df['phone_touchpoints'] = test_raw_df['touchpoints'].apply(\n",
    "    lambda x: x.count('phone') if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "phone_touch_30d = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30D', on='date')['phone_touchpoints']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='phone_touchpoints_30d')\n",
    ")\n",
    "\n",
    "# Fill NA values for customers with no support interactions\n",
    "email_csat_60d['avg_email_csat_60d'] = email_csat_60d['avg_email_csat_60d'].fillna(\n",
    "    test_raw_df['email_csat'].median()\n",
    ")\n",
    "complaints_90d['total_complaints_90d'] = complaints_90d['total_complaints_90d'].fillna(0)\n",
    "phone_touch_30d['phone_touchpoints_30d'] = phone_touch_30d['phone_touchpoints_30d'].fillna(0)\n",
    "\n",
    "print(\"Email CSAT (60d):\\n\", email_csat_60d.head())\n",
    "print(\"\\nComplaints (90d):\\n\", complaints_90d.head())\n",
    "print(\"\\nPhone Touchpoints (30d):\\n\", phone_touch_30d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# FINANCIAL FEATURES \n",
    "# ==================================================================\n",
    "# 1. Interest Rate Change (90d)\n",
    "# Ensure data is sorted chronologically per customer\n",
    "combined_train_raw_df = combined_train_raw_df.sort_values(['customer_id', 'date'])\n",
    "\n",
    "interest_rates = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('90D', on='date', closed='left')\n",
    "    ['interest_rate']\n",
    "    .mean()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='interest_rate_90d_ago')\n",
    ")\n",
    "\n",
    "current_interest = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')['interest_rate']\n",
    "    .last()\n",
    "    .reset_index(name='current_interest_rate')\n",
    ")\n",
    "\n",
    "interest_change = pd.merge(current_interest, interest_rates, on='customer_id')\n",
    "interest_change['interest_rate_change_90d'] = (\n",
    "    interest_change['current_interest_rate'] - interest_change['interest_rate_90d_ago']\n",
    ")\n",
    "\n",
    "# 2. Net Transfer Volume (30d)\n",
    "combined_train_raw_df['net_transfer_volume'] = (\n",
    "    combined_train_raw_df['bank_transfer_in_volume'] \n",
    "    - combined_train_raw_df['bank_transfer_out_volume']\n",
    ")\n",
    "\n",
    "net_transfer_30d = (\n",
    "    combined_train_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30D', on='date')\n",
    "    ['net_transfer_volume']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='net_transfer_30d')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# FINANCIAL FEATURES\n",
    "# ==================================================================\n",
    "# 1. Interest Rate Change (90d)\n",
    "# Ensure data is sorted chronologically per customer\n",
    "test_raw_df = test_raw_df.sort_values(['customer_id', 'date'])\n",
    "\n",
    "interest_rates = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('90D', on='date', closed='left')\n",
    "    ['interest_rate']\n",
    "    .mean()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='interest_rate_90d_ago')\n",
    ")\n",
    "\n",
    "current_interest = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')['interest_rate']\n",
    "    .last()\n",
    "    .reset_index(name='current_interest_rate')\n",
    ")\n",
    "\n",
    "interest_change = pd.merge(current_interest, interest_rates, on='customer_id')\n",
    "interest_change['interest_rate_change_90d'] = (\n",
    "    interest_change['current_interest_rate'] - interest_change['interest_rate_90d_ago']\n",
    ")\n",
    "\n",
    "# 2. Net Transfer Volume (30d)\n",
    "test_raw_df['net_transfer_volume'] = (\n",
    "    test_raw_df['bank_transfer_in_volume'] \n",
    "    - test_raw_df['bank_transfer_out_volume']\n",
    ")\n",
    "\n",
    "net_transfer_30d = (\n",
    "    test_raw_df\n",
    "    .groupby('customer_id')\n",
    "    .rolling('30D', on='date')\n",
    "    ['net_transfer_volume']\n",
    "    .sum()\n",
    "    .groupby('customer_id').last()\n",
    "    .reset_index(name='net_transfer_30d')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# TEMPORAL FEATURES\n",
    "# ==================================================================\n",
    "# Month of cutoff date\n",
    "df_train['month'] = cutoff_date.month\n",
    "\n",
    "# Tenure Buckets (using latest tenure)\n",
    "latest_tenure = combined_train_raw_df.groupby('customer_id')['tenure'].last().reset_index()\n",
    "latest_tenure['tenure_bucket'] = pd.cut(latest_tenure['tenure'],\n",
    "                                       bins=[0, 365, 1095, np.inf],\n",
    "                                       labels=[0, 1, 2]) #'<1 year' = 0, '1-3 years' = 1, '>3 years' = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# TEMPORAL FEATURES\n",
    "# ==================================================================\n",
    "# Month of cutoff date\n",
    "df_test['month'] = cutoff_date.month\n",
    "\n",
    "# Tenure Buckets (using latest tenure)\n",
    "latest_tenure = test_raw_df.groupby('customer_id')['tenure'].last().reset_index()\n",
    "latest_tenure['tenure_bucket'] = pd.cut(latest_tenure['tenure'],\n",
    "                                       bins=[0, 365, 1095, np.inf],\n",
    "                                       labels=[0, 1, 2]) #'<1 year' = 0, '1-3 years' = 1, '>3 years' = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# DERIVED METRICS\n",
    "# ==================================================================\n",
    "# Engagement Score (example calculation)\n",
    "engagement_components = pd.merge(\n",
    "    crypto_30d[['customer_id', '30d_crypto_volume']],\n",
    "    net_transfer_30d[['customer_id', 'net_transfer_30d']],\n",
    "    on='customer_id'\n",
    ")\n",
    "engagement_components['engagement_score'] = (\n",
    "    0.4 * engagement_components['30d_crypto_volume'] +\n",
    "    0.6 * engagement_components['net_transfer_30d']\n",
    ")\n",
    "\n",
    "# Risk Flag (Large Withdrawal - top 5%)\n",
    "withdrawal_threshold = combined_train_raw_df['bank_transfer_out_volume'].quantile(0.95)\n",
    "risk_flag = combined_train_raw_df[combined_train_raw_df['bank_transfer_out_volume'] > withdrawal_threshold]\\\n",
    "            .groupby('customer_id').size()\\\n",
    "            .reset_index(name='large_withdrawals')\n",
    "risk_flag['risk_flag'] = (risk_flag['large_withdrawals'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# DERIVED METRICS\n",
    "# ==================================================================\n",
    "# Engagement Score (example calculation)\n",
    "engagement_components = pd.merge(\n",
    "    crypto_30d[['customer_id', '30d_crypto_volume']],\n",
    "    net_transfer_30d[['customer_id', 'net_transfer_30d']],\n",
    "    on='customer_id'\n",
    ")\n",
    "engagement_components['engagement_score'] = (\n",
    "    0.4 * engagement_components['30d_crypto_volume'] +\n",
    "    0.6 * engagement_components['net_transfer_30d']\n",
    ")\n",
    "\n",
    "# Risk Flag (Large Withdrawal - top 5%)\n",
    "withdrawal_threshold = test_raw_df['bank_transfer_out_volume'].quantile(0.95)\n",
    "risk_flag = test_raw_df[test_raw_df['bank_transfer_out_volume'] > withdrawal_threshold]\\\n",
    "            .groupby('customer_id').size()\\\n",
    "            .reset_index(name='large_withdrawals')\n",
    "risk_flag['risk_flag'] = (risk_flag['large_withdrawals'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# FINAL MERGE\n",
    "# ==================================================================\n",
    "feature_list = [\n",
    "    crypto_30d, atm_trend_90d, last_transfer[['customer_id', 'days_since_transfer']],\n",
    "    email_csat_60d, complaints_90d, phone_touch_30d,\n",
    "    interest_change[['customer_id', 'interest_rate_change_90d']], \n",
    "    net_transfer_30d, latest_tenure[['customer_id', 'tenure_bucket']],\n",
    "    engagement_components[['customer_id', 'engagement_score']], \n",
    "    risk_flag[['customer_id', 'risk_flag']]\n",
    "]\n",
    "\n",
    "df_train = df_train.merge(feature_list[0], on='customer_id', how='left')\n",
    "for df in feature_list[1:]:\n",
    "    df_train = df_train.merge(df, on='customer_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "df_train.fillna({\n",
    "    '30d_crypto_volume': 0,\n",
    "    '90d_atm_trend': 0,\n",
    "    'days_since_transfer': (cutoff_date - pd.to_datetime('2000-01-01')).days,\n",
    "    'avg_email_csat_60d': df_train['avg_email_csat_60d'].median(),\n",
    "    'risk_flag': 0\n",
    "}, inplace=True)\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165155"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# FINAL MERGE\n",
    "# ==================================================================\n",
    "feature_list = [\n",
    "    crypto_30d, atm_trend_90d, last_transfer[['customer_id', 'days_since_transfer']],\n",
    "    email_csat_60d, complaints_90d, phone_touch_30d,\n",
    "    interest_change[['customer_id', 'interest_rate_change_90d']], \n",
    "    net_transfer_30d, latest_tenure[['customer_id', 'tenure_bucket']],\n",
    "    engagement_components[['customer_id', 'engagement_score']], \n",
    "    risk_flag[['customer_id', 'risk_flag']]\n",
    "]\n",
    "\n",
    "df_test = df_test.merge(feature_list[0], on='customer_id', how='left')\n",
    "for df in feature_list[1:]:\n",
    "    df_test = df_test.merge(df, on='customer_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "df_test.fillna({\n",
    "    '30d_crypto_volume': 0,\n",
    "    '90d_atm_trend': 0,\n",
    "    'days_since_transfer': (cutoff_date - pd.to_datetime('2000-01-01')).days,  # Arbitrary old date\n",
    "    'avg_email_csat_60d': df_test['avg_email_csat_60d'].median(),\n",
    "    'risk_flag': 0\n",
    "}, inplace=True)\n",
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_unemployed\n",
      "0    94780\n",
      "1    11399\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get latest job status for each customer\n",
    "latest_jobs = (\n",
    "    combined_train_raw_df\n",
    "    .sort_values(['customer_id', 'date'], ascending=[True, False])\n",
    "    .groupby('customer_id')['job']\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create binary flag for unemployed status (latest record only)\n",
    "latest_jobs['is_unemployed'] = (\n",
    "    latest_jobs['job']\n",
    "    .str.strip().str.lower()\n",
    "    .eq('unemployed')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Merge with training data\n",
    "df_train = df_train.merge(\n",
    "    latest_jobs[['customer_id', 'is_unemployed']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ").fillna({'is_unemployed': 0})\n",
    "\n",
    "print(df_train['is_unemployed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_unemployed\n",
      "0    135846\n",
      "1     29309\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get latest job status for each customer\n",
    "latest_jobs = (\n",
    "    test_raw_df\n",
    "    .sort_values(['customer_id', 'date'], ascending=[True, False])\n",
    "    .groupby('customer_id')['job']\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create binary flag for unemployed status (latest record only)\n",
    "latest_jobs['is_unemployed'] = (\n",
    "    latest_jobs['job']\n",
    "    .str.strip().str.lower()\n",
    "    .eq('unemployed')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Merge with testing data\n",
    "df_test = df_test.merge(\n",
    "    latest_jobs[['customer_id', 'is_unemployed']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ").fillna({'is_unemployed': 0})\n",
    "\n",
    "print(df_test['is_unemployed'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Train/Test sets to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>churn_status</th>\n",
       "      <th>month</th>\n",
       "      <th>30d_crypto_volume</th>\n",
       "      <th>90d_atm_trend</th>\n",
       "      <th>days_since_transfer</th>\n",
       "      <th>avg_email_csat_60d</th>\n",
       "      <th>total_complaints_90d</th>\n",
       "      <th>phone_touchpoints_30d</th>\n",
       "      <th>interest_rate_change_90d</th>\n",
       "      <th>net_transfer_30d</th>\n",
       "      <th>tenure_bucket</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>risk_flag</th>\n",
       "      <th>is_unemployed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1602.602938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-242.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>734.663785</td>\n",
       "      <td>2</td>\n",
       "      <td>1081.839446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>261.286543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-187.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.824867</td>\n",
       "      <td>1</td>\n",
       "      <td>106.209538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>8490.917518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40025.197105</td>\n",
       "      <td>2</td>\n",
       "      <td>27411.485270</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>766.619416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.017325</td>\n",
       "      <td>1</td>\n",
       "      <td>525.658162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>304.032077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2698.584398</td>\n",
       "      <td>2</td>\n",
       "      <td>1740.763470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  churn_status  month  30d_crypto_volume  90d_atm_trend  \\\n",
       "0            1         False      6        1602.602938            0.0   \n",
       "1            2          True      6         261.286543            0.0   \n",
       "2            3          True      6        8490.917518            0.0   \n",
       "3            4          True      6         766.619416            0.0   \n",
       "4            5         False      6         304.032077            0.0   \n",
       "\n",
       "   days_since_transfer  avg_email_csat_60d  total_complaints_90d  \\\n",
       "0               -242.0                 6.0                   0.0   \n",
       "1               -187.0                 6.0                   0.0   \n",
       "2                246.0                 6.0                   0.0   \n",
       "3                128.0                 6.0                   0.0   \n",
       "4                139.0                 6.0                   0.0   \n",
       "\n",
       "   phone_touchpoints_30d  interest_rate_change_90d  net_transfer_30d  \\\n",
       "0                    0.0                       NaN        734.663785   \n",
       "1                    0.0                       NaN          2.824867   \n",
       "2                    0.0                       NaN      40025.197105   \n",
       "3                    0.0                       NaN        365.017325   \n",
       "4                    0.0                       0.5       2698.584398   \n",
       "\n",
       "  tenure_bucket  engagement_score  risk_flag  is_unemployed  \n",
       "0             2       1081.839446        0.0              0  \n",
       "1             1        106.209538        0.0              0  \n",
       "2             2      27411.485270        1.0              0  \n",
       "3             1        525.658162        0.0              0  \n",
       "4             2       1740.763470        0.0              0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display final df_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>churn_status</th>\n",
       "      <th>month</th>\n",
       "      <th>30d_crypto_volume</th>\n",
       "      <th>90d_atm_trend</th>\n",
       "      <th>days_since_transfer</th>\n",
       "      <th>avg_email_csat_60d</th>\n",
       "      <th>total_complaints_90d</th>\n",
       "      <th>phone_touchpoints_30d</th>\n",
       "      <th>interest_rate_change_90d</th>\n",
       "      <th>net_transfer_30d</th>\n",
       "      <th>tenure_bucket</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>risk_flag</th>\n",
       "      <th>is_unemployed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1602.602938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-242.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>734.663785</td>\n",
       "      <td>2</td>\n",
       "      <td>1081.839446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>261.286543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-187.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.824867</td>\n",
       "      <td>1</td>\n",
       "      <td>106.209538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>8490.917518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40025.197105</td>\n",
       "      <td>2</td>\n",
       "      <td>27411.485270</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>766.619416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.017325</td>\n",
       "      <td>1</td>\n",
       "      <td>525.658162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>304.032077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2698.584398</td>\n",
       "      <td>2</td>\n",
       "      <td>1740.763470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  churn_status  month  30d_crypto_volume  90d_atm_trend  \\\n",
       "0            1         False      6        1602.602938            0.0   \n",
       "1            2         False      6         261.286543            0.0   \n",
       "2            3          True      6        8490.917518            0.0   \n",
       "3            4          True      6         766.619416            0.0   \n",
       "4            5          True      6         304.032077            0.0   \n",
       "\n",
       "   days_since_transfer  avg_email_csat_60d  total_complaints_90d  \\\n",
       "0               -242.0                 6.0                   0.0   \n",
       "1               -187.0                 6.0                   0.0   \n",
       "2                246.0                 6.0                   0.0   \n",
       "3                128.0                 6.0                   0.0   \n",
       "4                139.0                 6.0                   0.0   \n",
       "\n",
       "   phone_touchpoints_30d  interest_rate_change_90d  net_transfer_30d  \\\n",
       "0                    0.0                       NaN        734.663785   \n",
       "1                    0.0                       NaN          2.824867   \n",
       "2                    0.0                       NaN      40025.197105   \n",
       "3                    0.0                       NaN        365.017325   \n",
       "4                    0.0                       0.5       2698.584398   \n",
       "\n",
       "  tenure_bucket  engagement_score  risk_flag  is_unemployed  \n",
       "0             2       1081.839446        0.0              0  \n",
       "1             1        106.209538        0.0              0  \n",
       "2             2      27411.485270        1.0              0  \n",
       "3             1        525.658162        0.0              0  \n",
       "4             2       1740.763470        0.0              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset saved as: ../data/train_2025-01-30_20-06-53.csv\n"
     ]
    }
   ],
   "source": [
    "filename = f\"../data/train_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "df_train.to_csv(filename, index=False)\n",
    "print(f\"Trainset saved as: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset saved as: ../data/test_2025-01-30_20-06-54.csv\n"
     ]
    }
   ],
   "source": [
    "filename = f\"../data/test_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "df_test.to_csv(filename, index=False)\n",
    "print(f\"Testset saved as: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
