{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/train_2023.parquet\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['churn_due_to_fraud'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.countplot(x='churn_due_to_fraud', data=df)\n",
    "\n",
    "# Annotate each bar with its count\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2, \n",
    "            p.get_height() + 0.5,  # Position above the bar\n",
    "            int(p.get_height()),  # The count\n",
    "            ha='center')  # Center align the text\n",
    "\n",
    "plt.title('Churn Due to Fraud Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df[numeric_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2008, 2024)\n",
    "file_path_pattern = \"../data/train_{year}.parquet\"\n",
    "\n",
    "total_churn_due_to_fraud = 0\n",
    "total_records = 0\n",
    "\n",
    "for year in years:\n",
    "    file_path = file_path_pattern.format(year=year)\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        year_total_records = len(df)\n",
    "        total_records += year_total_records\n",
    "\n",
    "        churn_count = df['churn_due_to_fraud'].sum()\n",
    "        total_churn_due_to_fraud += churn_count\n",
    "\n",
    "        print(f\"Year {year}: {churn_count} records (Total records: {year_total_records})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file for year {year}: {e}\")\n",
    "\n",
    "print(f\"Total records across all datasets: {total_records}\")\n",
    "print(f\"Total records where 'churn_due_to_fraud' is True: {total_churn_due_to_fraud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.drop(columns=['Id', 'name', 'address'])\n",
    "df_features['date_of_birth'] = pd.to_datetime(df_features['date_of_birth'])\n",
    "df_features['date'] = pd.to_datetime(df_features['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1: Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['age'] = (df_features['date'] - df_features['date_of_birth']).dt.days // 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate transaction frequencies (how many transactions were made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['atm_transfer_frequency'] = df_features['atm_transfer_in'] + df_features['atm_transfer_out']\n",
    "df_features['bank_transfer_frequency'] = df_features['bank_transfer_in'] + df_features['bank_transfer_out']\n",
    "df_features['crypto_transfer_frequency'] = df_features['crypto_in'] + df_features['crypto_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ratio of incoming to outgoing transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['atm_in_out_ratio'] = df_features['atm_transfer_in'] / (df_features['atm_transfer_out'] + 1)\n",
    "df_features['bank_in_out_ratio'] = df_features['bank_transfer_in'] / (df_features['bank_transfer_out'] + 1)\n",
    "df_features['crypto_in_out_ratio'] = df_features['crypto_in'] / (df_features['crypto_out'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For categorical features (country, touchpoints, csat_scores, job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_parquet(\"../data/train_2023.parquet\")\n",
    "test_data = pd.read_parquet('../data/test.parquet')\n",
    "\n",
    "# Check for missing values\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Convert date column to datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Feature Engineering: Create age from date_of_birth\n",
    "train_data['date_of_birth'] = pd.to_datetime(train_data['date_of_birth'])\n",
    "train_data['age'] = (train_data['date'] - train_data['date_of_birth']).dt.days // 365\n",
    "\n",
    "test_data['date_of_birth'] = pd.to_datetime(test_data['date_of_birth'])\n",
    "test_data['age'] = (test_data['date'] - test_data['date_of_birth']).dt.days // 365\n",
    "\n",
    "# Day of the week, month, and year features\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "\n",
    "test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "\n",
    "# Calculate the number of touchpoints per day\n",
    "train_data['touchpoints_count'] = train_data['touchpoints'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "test_data['touchpoints_count'] = test_data['touchpoints'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "\n",
    "# Calculate the average CSAT score (assuming CSAT scores are stored as a dictionary-like string)\n",
    "# We need to convert the csat_scores column from string to actual dictionary type for calculation\n",
    "import ast\n",
    "\n",
    "def get_avg_csat_score(csat_str):\n",
    "    try:\n",
    "        csat_dict = ast.literal_eval(csat_str)  # convert string to dictionary\n",
    "        return np.mean(list(csat_dict.values())) if csat_dict else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "train_data['avg_csat_score'] = train_data['csat_scores'].apply(get_avg_csat_score)\n",
    "test_data['avg_csat_score'] = test_data['csat_scores'].apply(get_avg_csat_score)\n",
    "\n",
    "# Transaction features: create summary statistics for transactions\n",
    "def create_transaction_features(df):\n",
    "    df['atm_transfer_in_total'] = df['atm_transfer_in'] + df['atm_transfer_out']\n",
    "    df['bank_transfer_total'] = df['bank_transfer_in'] + df['bank_transfer_out']\n",
    "    df['crypto_transfer_total'] = df['crypto_in'] + df['crypto_out']\n",
    "    df['bank_transfer_volume'] = df['bank_transfer_in_volume'] + df['bank_transfer_out_volume']\n",
    "    df['crypto_transfer_volume'] = df['crypto_in_volume'] + df['crypto_out_volume']\n",
    "    return df\n",
    "\n",
    "train_data = create_transaction_features(train_data)\n",
    "test_data = create_transaction_features(test_data)\n",
    "\n",
    "# Rolling averages (e.g., over the last 7 days) for transaction volume\n",
    "train_data['rolling_7_days_bank_transfer_volume'] = train_data['bank_transfer_volume'].rolling(7).mean()\n",
    "test_data['rolling_7_days_bank_transfer_volume'] = test_data['bank_transfer_volume'].rolling(7).mean()\n",
    "\n",
    "train_data['rolling_7_days_crypto_transfer_volume'] = train_data['crypto_transfer_volume'].rolling(7).mean()\n",
    "test_data['rolling_7_days_crypto_transfer_volume'] = test_data['crypto_transfer_volume'].rolling(7).mean()\n",
    "\n",
    "# Feature encoding for categorical variables (one-hot encoding)\n",
    "categorical_cols = ['country', 'job', 'from_competitor']\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=True)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Drop non-essential columns\n",
    "drop_cols = ['Id', 'name', 'address', 'date_of_birth', 'date', 'csat_scores', 'customer_id']\n",
    "train_data = train_data.drop(columns=drop_cols)\n",
    "test_data = test_data.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(train_data.columns):\n",
    "    print(f\"{i} {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
