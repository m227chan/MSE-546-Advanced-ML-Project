{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the years 2008 to 2023\n",
    "for year in range(2008, 2024):\n",
    "    # Construct the file path for the current year\n",
    "    file_path = f\"../data/train_{year}.parquet\"\n",
    "    \n",
    "    # Read the Parquet file into a DataFrame and append it to the list\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read file {file_path}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"Total rows in the combined DataFrame: {len(combined_df)}\")\n",
    "else:\n",
    "    print(\"No files were successfully read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['customer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the most recent date in the dataset\n",
    "max_date = combined_df['date'].max()\n",
    "\n",
    "# Step 2: Group by customer_id to get the latest event date for each customer\n",
    "latest_event = combined_df.groupby('customer_id')['date'].max().reset_index()\n",
    "\n",
    "# Step 3: Calculate churn condition\n",
    "# A customer is considered churned if their last event is older than 12 months from max_date\n",
    "latest_event['churn'] = latest_event['date'] < (max_date - pd.DateOffset(months=12))\n",
    "\n",
    "# Step 4: Merge this back into the original dataframe\n",
    "combined_df = pd.merge(combined_df, latest_event[['customer_id', 'churn']], on='customer_id', how='left')\n",
    "\n",
    "# Now, the combined_df will have the new 'churn' column\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct customer_ids where churn is True or False\n",
    "churn_true_count = combined_df[combined_df['churn'] == True]['customer_id'].nunique()\n",
    "churn_false_count = combined_df[combined_df['churn'] == False]['customer_id'].nunique()\n",
    "\n",
    "print(f\"Number of distinct customers with churn = True: {churn_true_count}\")\n",
    "print(f\"Number of distinct customers with churn = False: {churn_false_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[combined_df['churn'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Churn member: customer_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[combined_df['customer_id'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(combined_df[combined_df['customer_id'] == 1]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(combined_df[combined_df['customer_id'] == 1]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the DataFrame for customer_id = 1\n",
    "customer_data = combined_df[combined_df['customer_id'] == 2]\n",
    "\n",
    "# Calculate the total amount in and out over time\n",
    "customer_data['total_in'] = customer_data['atm_transfer_in'] + customer_data['bank_transfer_in'] + customer_data['crypto_in']\n",
    "customer_data['total_out'] = customer_data['atm_transfer_out'] + customer_data['bank_transfer_out'] + customer_data['crypto_out']\n",
    "\n",
    "# Group by date to sum the total in and out amounts per day\n",
    "daily_data = customer_data.groupby('date')[['total_in', 'total_out']].sum()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_data.index, daily_data['total_in'], label='Total In', color='green')\n",
    "plt.plot(daily_data.index, daily_data['total_out'], label='Total Out', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Total Amount In and Out for Customer ID = 1 Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend()\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=combined_df, x='country', order=combined_df['country'].value_counts().index)\n",
    "plt.title('Count of Entries by Country', fontsize=16)\n",
    "plt.xlabel('Country', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the DataFrame to include only rows where churn_due_to_fraud is True\n",
    "churn_true_df = combined_df[combined_df['churn_due_to_fraud'] == True]\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the countplot for filtered data\n",
    "sns.countplot(\n",
    "    data=churn_true_df,\n",
    "    x='country',\n",
    "    order=churn_true_df['country'].value_counts().index,\n",
    "    color='red'  # Color the bars red\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Count of Entries by Country (Churn Due to Fraud Only)', fontsize=16)\n",
    "plt.xlabel('Country', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[combined_df[\"customer_id\"] == 3367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(combined_df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['churn_due_to_fraud'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.countplot(x='churn_due_to_fraud', data=df)\n",
    "\n",
    "# Annotate each bar with its count\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2, \n",
    "            p.get_height() + 0.5,  # Position above the bar\n",
    "            int(p.get_height()),  # The count\n",
    "            ha='center')  # Center align the text\n",
    "\n",
    "plt.title('Churn Due to Fraud Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df[numeric_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2008, 2024)\n",
    "file_path_pattern = \"../data/train_{year}.parquet\"\n",
    "\n",
    "total_churn_due_to_fraud = 0\n",
    "total_records = 0\n",
    "\n",
    "for year in years:\n",
    "    file_path = file_path_pattern.format(year=year)\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        year_total_records = len(df)\n",
    "        total_records += year_total_records\n",
    "\n",
    "        churn_count = df['churn_due_to_fraud'].sum()\n",
    "        total_churn_due_to_fraud += churn_count\n",
    "\n",
    "        print(f\"Year {year}: {churn_count} records (Total records: {year_total_records})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file for year {year}: {e}\")\n",
    "\n",
    "print(f\"Total records across all datasets: {total_records}\")\n",
    "print(f\"Total records where 'churn_due_to_fraud' is True: {total_churn_due_to_fraud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.drop(columns=['Id', 'name', 'address'])\n",
    "df_features['date_of_birth'] = pd.to_datetime(df_features['date_of_birth'])\n",
    "df_features['date'] = pd.to_datetime(df_features['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1: Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['age'] = (df_features['date'] - df_features['date_of_birth']).dt.days // 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate transaction frequencies (how many transactions were made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['atm_transfer_frequency'] = df_features['atm_transfer_in'] + df_features['atm_transfer_out']\n",
    "df_features['bank_transfer_frequency'] = df_features['bank_transfer_in'] + df_features['bank_transfer_out']\n",
    "df_features['crypto_transfer_frequency'] = df_features['crypto_in'] + df_features['crypto_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ratio of incoming to outgoing transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['atm_in_out_ratio'] = df_features['atm_transfer_in'] / (df_features['atm_transfer_out'] + 1)\n",
    "df_features['bank_in_out_ratio'] = df_features['bank_transfer_in'] / (df_features['bank_transfer_out'] + 1)\n",
    "df_features['crypto_in_out_ratio'] = df_features['crypto_in'] / (df_features['crypto_out'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For categorical features (country, touchpoints, csat_scores, job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_parquet(\"../data/train_2023.parquet\")\n",
    "test_data = pd.read_parquet('../data/test.parquet')\n",
    "\n",
    "# Check for missing values\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Convert date column to datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Feature Engineering: Create age from date_of_birth\n",
    "train_data['date_of_birth'] = pd.to_datetime(train_data['date_of_birth'])\n",
    "train_data['age'] = (train_data['date'] - train_data['date_of_birth']).dt.days // 365\n",
    "\n",
    "test_data['date_of_birth'] = pd.to_datetime(test_data['date_of_birth'])\n",
    "test_data['age'] = (test_data['date'] - test_data['date_of_birth']).dt.days // 365\n",
    "\n",
    "# Day of the week, month, and year features\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "\n",
    "test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "\n",
    "# Calculate the number of touchpoints per day\n",
    "train_data['touchpoints_count'] = train_data['touchpoints'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "test_data['touchpoints_count'] = test_data['touchpoints'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "\n",
    "# Calculate the average CSAT score (assuming CSAT scores are stored as a dictionary-like string)\n",
    "# We need to convert the csat_scores column from string to actual dictionary type for calculation\n",
    "import ast\n",
    "\n",
    "def get_avg_csat_score(csat_str):\n",
    "    try:\n",
    "        csat_dict = ast.literal_eval(csat_str)  # convert string to dictionary\n",
    "        return np.mean(list(csat_dict.values())) if csat_dict else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "train_data['avg_csat_score'] = train_data['csat_scores'].apply(get_avg_csat_score)\n",
    "test_data['avg_csat_score'] = test_data['csat_scores'].apply(get_avg_csat_score)\n",
    "\n",
    "# Transaction features: create summary statistics for transactions\n",
    "def create_transaction_features(df):\n",
    "    df['atm_transfer_in_total'] = df['atm_transfer_in'] + df['atm_transfer_out']\n",
    "    df['bank_transfer_total'] = df['bank_transfer_in'] + df['bank_transfer_out']\n",
    "    df['crypto_transfer_total'] = df['crypto_in'] + df['crypto_out']\n",
    "    df['bank_transfer_volume'] = df['bank_transfer_in_volume'] + df['bank_transfer_out_volume']\n",
    "    df['crypto_transfer_volume'] = df['crypto_in_volume'] + df['crypto_out_volume']\n",
    "    return df\n",
    "\n",
    "train_data = create_transaction_features(train_data)\n",
    "test_data = create_transaction_features(test_data)\n",
    "\n",
    "# Rolling averages (e.g., over the last 7 days) for transaction volume\n",
    "train_data['rolling_7_days_bank_transfer_volume'] = train_data['bank_transfer_volume'].rolling(7).mean()\n",
    "test_data['rolling_7_days_bank_transfer_volume'] = test_data['bank_transfer_volume'].rolling(7).mean()\n",
    "\n",
    "train_data['rolling_7_days_crypto_transfer_volume'] = train_data['crypto_transfer_volume'].rolling(7).mean()\n",
    "test_data['rolling_7_days_crypto_transfer_volume'] = test_data['crypto_transfer_volume'].rolling(7).mean()\n",
    "\n",
    "# Feature encoding for categorical variables (one-hot encoding)\n",
    "categorical_cols = ['country', 'job', 'from_competitor']\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=True)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Drop non-essential columns\n",
    "drop_cols = ['Id', 'name', 'address', 'date_of_birth', 'date', 'csat_scores', 'customer_id']\n",
    "train_data = train_data.drop(columns=drop_cols)\n",
    "test_data = test_data.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(train_data.columns):\n",
    "    print(f\"{i} {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
